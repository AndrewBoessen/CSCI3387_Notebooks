{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yCZvOywlqXzZ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewBoessen/CSCI3387_Notebooks/blob/main/Milestone_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CSCI3387 Project Milestone #2\n",
        "_Andrew Boessen, Ian Bourgin, Theodore Grace_"
      ],
      "metadata": {
        "id": "uRZufhtcmPAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Simulating Real-Time Game Environments with Neural Networks__\n",
        "\n",
        "## Project Objective / Definition:\n",
        "\n",
        "Video games are defined by a _game loop_ where the system (1) collect user inputs, (2) updates the game state, and (3) renders screen pixels.\n",
        "\n",
        "An interactive game environment $\\mathcal E$ consists of a latent space $\\mathcal S$ of states, a set of actions $\\mathcal A$, a space of observations $\\mathcal O$, a projection function $V : \\mathcal S â†’ \\mathcal O$, and a transition function $p(s|s',a)$ where $s',s \\in \\mathcal S, a \\in \\mathcal A$\n",
        "\n",
        "Our project aims to simulate a game environment with a nueral network. This can be defined as an _Interactive World Simulation_ which given an environment $\\mathcal E$ and an initial state $s_0 \\in \\mathcal S$ is a distribution function $q(o_n|o_{<n}, a_{<n}), o_i \\in \\mathcal O, a_i \\in \\mathcal A$. The Interactive World Simulation objective consists of minimizing $\\mathbb{E}(D(o^i_q, o^i_p))$, where $D : \\mathcal O \\times \\mathcal O \\rightarrow \\mathbb{R}$ is a distance metric between observations and $o^i_q \\sim q$, $o^i_p \\sim V(p)$.\n",
        "\n",
        "## Atari Skiing\n",
        "\n",
        "For our project, we have chosen to simulate _Skiing_, which is a game released for the Atari 2600. _Skiing_ is a simple game where the objective is to reach the bottom of the ski course in the least amount of time. To reach the bottom of the course, players must dodge obstacles and pass through a series of gates, indicated by flagpoles.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1aVJjOk12eTrPt7HU0Sc-5dMra27mSD8t\" height=\"300\">\n",
        "</div>\n",
        "\n",
        "We chose this game becaue of its simple graphics, small environment, and limited set of actions. In the case of _Skiing_, the environment $\\mathcal E$ is defined where $\\mathcal S$ is the programs dynamic memory content, i.e game variables, $\\mathcal O$ is the rendered screen pixels, $V$ is the games rendering logic, $\\mathcal A$ contains three actions: [move left, move right, no action], and $p$ is the games logic.\n",
        "\n",
        "_Skiing_ contains ten levels of varying difficulty and length. These levels are split between those containing gates and those without. For more imformation about _Skiing_ see [the AtariAge page](https://atariage.com/manual_html_page.php?SoftwareLabelID=434).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e3FcpHLNmvhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Model Architecture__\n",
        "\n",
        "In order to simulate the game environment, we use a spatiotemporal (ST)\n",
        " transformer [(Xu et al. 2021)](https://arxiv.org/pdf/2001.02908) to autoregressivly model the game's environment based on a single stream of action and image tokens. Following [(Ramesh et al., 2021)](https://arxiv.org/pdf/2102.12092), we use a Vector Quantized Variational Autoencoder (VQ-VAE) [ (Van Den Oord et al., 2017)](https://arxiv.org/pdf/1711.00937) to encode images into a discrete latent space, and a decoder-only transformer to autoregressively predict the next frame using MaskGIT [(Chang et al. 2022)](https://arxiv.org/pdf/2202.04200). We then use the decoder from the VQ-VAE to obtain a new image which is presented to the user as the next frame in the video game simulation. Our task is very similar to video generation and we implement a similar architecture to VideoGPT [(Yan et al. 2021)](https://arxiv.org/pdf/2104.10157) by using a VQ-VAE and transformer to generate sequences of images. Our model differs in that it is real-time video generation (~20fps), and because our model requires a sequence of actions which are only available throughout the generation, our model must generate video autoregressively.\n",
        "\n",
        "We chose this approach becasue it allows us to generate long sequences of images while maintaining continuity between frames and respecting user inputs. For example, the transformer can attend to tokens from past images within it's context window. Compared to GameNGen [(Valevski et al., 2024)](https://arxiv.org/pdf/2408.14837) which ultizlies a computationaly expensive diffusion model to simulate the game environment, our approach will be able to generate more images-per-second which enables real-time interaction with the simulation. The choice of encoding images with a VQ-VAE allows the transformer to work in a lower dimensional latent space, which also reduces the computational complexity needed to generate an image. Additionaly, compared to the quadratic memory cost of a traditional transformer [(Vaswani et al. 2017)](https://arxiv.org/pdf/1706.03762), the ST-transfomer is memory efficient and balances model performace with computational constraints.\n",
        "\n",
        "## Image Encoder (VQ-VAE)\n",
        "\n",
        "The VQ-VAE network is an encoder-decoder network that encodes an image into a discrete latent space. This network is closely realted to a VAE network. A VAE consists of an encoder which paramterizes a posterior distribution $q(z|x)$ of the latent random variable $z$ given input $x$, a prior distribution $p(z)$, and a decoder with a distribution $p(x|z)$ over the input data. Typically, the posterior and prior distributions are normally distributed, but the VQ-VAE network uses vector quantization to extract prior and posterior distributions that are categorical. Samples are then drawn from these distributions and index an embedding table. These embeddings are then used as inputs to the decoder network.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1icVx2q_agOGYHNsDEDSQqZq5U5X6QPxi\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "The latent embedding space $e$ is defined where $e \\in R^{K \\times D}$ and $K$ is the size of the discrete latent space (i.e. a $K$-way categorical distribution), and $D$ is the embedding dimension. To produce a latent encoding, the encoder takes and input $x$ and produces and output $z_e(x)$. A nearest neighbor look-up using the shared embedding space $e$ calculates the index $i$ of the embedding. An embedding table is used to obtain the embedding $e_i$ where $1 \\leq i \\leq K$.\n",
        "\n",
        "The posterior categorical distribution $q(z|x)$ is defined as a one-hot encoding:\n",
        "\n",
        "$$q(z=k|x) = \\begin{cases}\n",
        "1, & \\text{for } k = \\text{argmin}_j ||z_e(x)-e_j||_2 \\\\\n",
        "0, & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "where $z_e(x)$ is the output of the encoder network. This output is passed through the discretisation bottleneck and it mapped to the nearest embedding. This function $z_q(x)$ is described as:\n",
        "\n",
        "$$z_q(x) = e_k,\n",
        "\\text{where } k = \\text{argmin}_j ||z_e(x) - e_j||_2$$\n",
        "\n",
        "## Generative ST-Transformer Model\n",
        "\n",
        "In our model, we use a transformer to generate new images by autorgressively sampling from the latent space $z$. Insipred by Genie, [(Brude et al. 2024)](https://arxiv.org/pdf/2402.15391) we use a ST-Transformer for this task.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1SJl-txgiz32S9ZTLlRPGq-AKL8U2hkpS\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "Unlike a traditional transformer where every token attends to all others, an ST-transformer contains $L$ spatiotemporal blocks with interleaved spatial and temporal attention layers, followed by a feed-forward layer (FFW) as standard attention blocks. The self-attention in the spatial layer attends over the $1 \\times H \\times W$ tokens within each time step, and in the temporal layer attends over $T \\times 1 \\times 1$ tokens across the $T$ time steps. Crucially, the dominating factor of computation complexity (i.e. the spatial attention layer) in this architecture scales linearly with the number of frames rather than quadratically, making it much more efficient for video generation with consistent dynamics over extended interactions.\n",
        "\n"
      ],
      "metadata": {
        "id": "ud50NhpPVIJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Data Collection via RL Agent__\n",
        "\n",
        "The goal of our project is to have a simulation that human players can interact with in real-time. Becasue of this, our training data should reflect _human gameplay_. Unfortantely, there is not a large uniform dataset of huam gameplay of Atari games, so we will have to find a alternate source of data to approximate human gameplay. Similar to GameNGen, [(Valevski et al., 2024)](https://arxiv.org/pdf/2408.14837) we train a RL agent to collect sequences of gameplay data by interacting with a game environment. Unlike, a typical RL setup which attemps to maximize a game score, our goal is to generate training data. This means that the data the RL agent collects should be diverse and contain a variety of scenarios to maximize training data efficency.\n",
        "\n",
        "Due to the simple nature of the game we chose, _Skiing_, we can use a very simple reward function to achieve this goal. The goal of the game is to reach the end of the course in the least amount of time possible, so the reward for our model is the in-game time. To build a complete dataset, we record all of the agent's training trajectories throughout the entire training process.\n",
        "\n",
        "We use the [Gymnasium](https://gymnasium.farama.org/index.html) library for the game environment that the rl agent collects data from. This library include many Atari game environments including [Skiing](https://gymnasium.farama.org/environments/atari/skiing/). This environment is defined where the action space $\\mathcal A$ contains 3 discrete actions, and the observation space $\\mathcal O = [0, 255]^{H \\times W \\times C}$ where $H = 210, W = 160, C = 3$. As it trains, the agents collects sequences of observations, which we then use as training data for the generative model.\n",
        "\n",
        "## RL Agent Method"
      ],
      "metadata": {
        "id": "T-_EpvGizMBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Re: Milestone #1__\n",
        "\n",
        "Based on feedback from Milestone #1, we have changed mutiple parts of our models architecture, but have kept the same objective for the project as a whole. In our last Milestone, we proposed to simulate the game DOOM using a diffusion model trained on data collected from the a RL agent. Due to concerns about the complexity of DOOM's game environment and the computationaly complexity of a diffusion model, we have decided to instead simulate a simpler Atari game, and have modified the architecture to fit within the computational constraints. Above, we have proposed a new architecture, that differs from GameNGen and we justify why this new architecture will be better fit for our objective. We have also chosen to train the RL agent using Deep Q Learning, rather than the Proximal Policy Optimization algorithm used in GameNGen."
      ],
      "metadata": {
        "id": "l_SyH8-1Lymk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Contribution__\n",
        "\n",
        "## Andrew Boessen\n",
        "\n",
        "- Researched new architectural changes from Milestone #1\n",
        "- Wrote report on Project Definition, Model Architecture, VQ-VAE, ST-Transformer, and Data Collection\n",
        "\n",
        "## Ian Bourgin\n",
        "\n",
        "- Researched RL methods and data collection\n",
        "- Create pytorch implementation of Deep-Q-Learning\n",
        "\n",
        "## Theodore Grace\n",
        "\n",
        "- Created pytorch implementation of VQ-VAE network"
      ],
      "metadata": {
        "id": "8vaZ6KB3zUIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _Dependencies_"
      ],
      "metadata": {
        "id": "p-2tq2m8sxOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install umap\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOnUIXjas9mZ",
        "outputId": "650970b5-4b07-4a6f-bf4f-7a105b291843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: umap in /usr/local/lib/python3.10/dist-packages (0.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# _Imports_"
      ],
      "metadata": {
        "id": "o_bodAQGs4bD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import xrange\n",
        "import umap\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython import display\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ],
      "metadata": {
        "id": "-BUFPj4gtH8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __VQ-VAE Implementation__\n",
        "\n",
        "The pytorch implementation for the VQ-VAE network is given below"
      ],
      "metadata": {
        "id": "yCZvOywlqXzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorQuantizerEMA(nn.Module):\n",
        "    '''\n",
        "    Implements Exponential Moving Average (EMA) vector quantization for VQ-VAE models.\n",
        "    '''\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim # Dimension of the embedding, D\n",
        "        self._num_embeddings = num_embeddings # Number of categories in distribution, K\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim), # Embedding table\n",
        "        self._embedding.weight.data.normal_()\n",
        "        self._commitment_cost = commitment_cost # Constant used in loss function\n",
        "\n",
        "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
        "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
        "        self._ema_w.data.normal_()\n",
        "\n",
        "        self._decay = decay\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Use EMA to update the embedding vectors\n",
        "        if self.training:\n",
        "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
        "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
        "\n",
        "            # Laplace smoothing of the cluster size\n",
        "            n = torch.sum(self._ema_cluster_size.data)\n",
        "            self._ema_cluster_size = (\n",
        "                (self._ema_cluster_size + self._epsilon)\n",
        "                / (n + self._num_embeddings * self._epsilon) * n)\n",
        "\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
        "\n",
        "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        loss = self._commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight Through Estimator\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
      ],
      "metadata": {
        "id": "nrU9uGLKiKjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super(Residual, self).__init__()\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=in_channels,\n",
        "                      out_channels=num_residual_hiddens,\n",
        "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=num_residual_hiddens,\n",
        "                      out_channels=num_hiddens,\n",
        "                      kernel_size=1, stride=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self._block(x)\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(ResidualStack, self).__init__()\n",
        "        self._num_residual_layers = num_residual_layers\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
        "                             for _ in range(self._num_residual_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self._num_residual_layers):\n",
        "            x = self._layers[i](x)\n",
        "        return F.relu(x)"
      ],
      "metadata": {
        "id": "l9DIavzLEV-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self._conv1 = nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens//2,\n",
        "                                 kernel_size=4,\n",
        "                                 stride=2, padding=1)\n",
        "\n",
        "        self._conv2 = nn.Conv2d(in_channels=num_hiddens//2, out_channels=num_hiddens,\n",
        "                                 kernel_size=4,\n",
        "                                 stride=2, padding=1)\n",
        "\n",
        "        self._conv3 = nn.Conv2d(in_channels=num_hiddens, out_channels=num_hiddens,\n",
        "                                 kernel_size=3,\n",
        "                                 stride=1, padding=1)\n",
        "\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
        "                                             num_hiddens=num_hiddens,\n",
        "                                             num_residual_layers=num_residual_layers,\n",
        "                                             num_residual_hiddens=num_residual_hiddens)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._conv1(inputs)\n",
        "        x = F.relu(x)\n",
        "        x = self._conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self._conv3(x)\n",
        "        return self._residual_stack(x)"
      ],
      "metadata": {
        "id": "BSkvptaljC94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self._conv1 = nn.Conv2d(in_channels=in_channels, out_channels=num_hiddens,\n",
        "                                kernel_size=3, stride=1, padding=1)\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens, num_hiddens=num_hiddens,\n",
        "                                             num_residual_layers=num_residual_layers,\n",
        "                                             num_residual_hiddens=num_residual_hiddens)\n",
        "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens,\n",
        "                                                out_channels=num_hiddens // 2,\n",
        "                                                kernel_size=4, stride=2, padding=1)\n",
        "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens // 2,\n",
        "                                                out_channels=3,\n",
        "                                                kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._conv1(inputs)\n",
        "        x = self._residual_stack(x)\n",
        "        x = self._conv_trans_1(x)\n",
        "        x = F.relu(x)\n",
        "        return self._conv_trans_2(x)"
      ],
      "metadata": {
        "id": "gIsh6u3ekGpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
        "                 num_embeddings, embedding_dim, commitment_cost, decay):\n",
        "        super(VQVAE, self).__init__()\n",
        "\n",
        "        self._encoder = Encoder(3, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens, out_channels=embedding_dim,\n",
        "                                      kernel_size=1, stride=1)\n",
        "\n",
        "        self._vq = VectorQuantizerEMA(num_embeddings, embedding_dim, commitment_cost, decay)\n",
        "        self._decoder = Decoder(embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self._encoder(x) # encode image to latent\n",
        "        z = self._pre_vq_conv(z)\n",
        "        loss, quantized, perplexity, _ = self._vq(z) # quantize encoding to dicrete space\n",
        "        x_recon = self._decoder(quantized) # reconstruction of input from decoder\n",
        "        return loss, x_recon, perplexity"
      ],
      "metadata": {
        "id": "2a8cH_PxHOKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RL Implementation (in progress)\n",
        "\n"
      ],
      "metadata": {
        "id": "DhepAVfYR46N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deep Q Network\n",
        "#\n",
        "# This model is used to approximate the Q function\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "        self.features = nn.Sequential(\n",
        "            # Extract feature vector from input observation\n",
        "            # This uses multiple CNN layers to get a feature representation\n",
        "            nn.Conv2d(input_shape[2], 32, kernel_size=8, stride=4), # Convert from 3 to 32 channels, downsample by 4x\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2), # Convert from 32 to 64 channels, downsample by 2x\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten() # Flatten 2d grid to vector\n",
        "        )\n",
        "\n",
        "        # Calculate the size of the output from the convolutional layers\n",
        "        conv_out_size = self._get_conv_out_size(input_shape)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512), # Linear projection to feature space\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions) # Lineat projection to action space\n",
        "        )\n",
        "\n",
        "    def _get_conv_out_size(self, shape):\n",
        "        o = self.features(torch.zeros(1, shape[2], shape[0], shape[1]))\n",
        "        return int(torch.prod(torch.tensor(o.size()))) # Get size of flattened vector\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2) # Permute to (batch_size, channels, height, width)\n",
        "        features = self.features(x)\n",
        "        return self.fc(features)"
      ],
      "metadata": {
        "id": "j02EdNjQR3-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(self,mini_batch,policy_network,target_network):\n",
        "\n",
        "  current_q_list = []\n",
        "  target_q_list = []\n",
        "\n",
        "  for state,action,new_state,reward,terminated in mini_batch:##optimize using replay memory mini batch\n",
        "\n",
        "    # This if/else block is to calculate the output of the target network\n",
        "    if terminated:\n",
        "      target = torch.FloatTensor([reward])\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        target = torch.FloatTensor(reward + self.discount_factor * torch.max(target_network(new_state))) #not sure about this\n",
        "\n",
        "    #Get the Q-values\n",
        "    current_q = policy_network(state)\n",
        "    current_q_list.append(current_q)\n",
        "\n",
        "    #Get the target values\n",
        "    target_q = target_network(state) #this value should be the same as current_q as networks are the same\n",
        "    target_q[action] = target  #replace\n",
        "    target_q_list.append(target_q)\n",
        "\n",
        "\n",
        "    ##compute loss for mini batch\n",
        "    loss = self.loss(torch.stack(current_q_list),torch.stack(target_q_list))\n",
        "\n",
        "    ##optimize policy network\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "AmFUUuRhSnbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining memory class for experience replay\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "class ReplayMemory(): # Deque containing some past experience where experience is a tuple (state,action,new_state,reward,terminated)\n",
        "  def __init__(self, maxlen):\n",
        "    self.memory = deque([],maxlen)\n",
        "\n",
        "  def append(self, experience): #add experience to deque\n",
        "    self.memory.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):  #return a random sample of experience, useful to ensure non correlation as experience is time-correlated\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def __len__(self): #returns length of deque\n",
        "    return len(self.memory)\n",
        "\n",
        "class SkiingDQL():\n",
        "  #Hyperparameter: turn this into function arg later\n",
        "  lr = 0.001\n",
        "  discount_factor = 0.9\n",
        "  network_sync_rate = 10 #number of steps agent takes before we sync the policy and target networks\n",
        "  replay_memeory_size = 1000\n",
        "  mini_batch_size = 32 #refers to size of batch sampled from replay memory\n",
        "\n",
        "  loss = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam\n",
        "\n",
        "  ACTIONS = [\"Left\",\"Right\",\"No_Action\"]\n",
        "\n",
        "  def train(self, episodes):\n",
        "    env = gym.make('ALE/Skiing-v5', render_mode=\"rgb_array\")\n",
        "    num_states = env.observation_space.n\n",
        "    num_actions =3\n",
        "    input_space = env.observation_space.shape\n",
        "\n",
        "    epsilon = 1 # fully random actions\n",
        "    memory = ReplayMemory(self.replay_memeory_size) #creating memory of experience\n",
        "\n",
        "    #Create the two networks\n",
        "    policy_network = DQN(input_space, num_actions)\n",
        "    target_network = DQN(input_space, num_actions)\n",
        "\n",
        "    target_network.load_state_dict(policy_network.state_dict()) #Copying the weights so that the networks are the same\n",
        "\n",
        "    # Keep track of rewards collected per episode --> Only sync networks if you have a reward, otherwise no learning will occur\n",
        "    rewards_per_episode = np.zeros(episodes)\n",
        "\n",
        "    #Keep track of epsilon decay  #The point of e-greedy algorithm is to choose the best action but also allow for exploration\n",
        "    epsilon_decays = []\n",
        "\n",
        "    #track number of steps taken to know when to sync networks\n",
        "    step_count = 0\n",
        "\n",
        "    for i in range(episodes):\n",
        "      state =env.reset()\n",
        "      terminated = False  #True when agent wins i.e goes through 20 poles\n",
        "      truncated = False #True when agent takes more than 200 actions (prevent infinite loop, unlikely)\n",
        "\n",
        "    #Picking action based on greedy algo -- Starts with full exploration --> choosing best action towards the end\n",
        "      if random.random() < epsilon:\n",
        "        action = env.action_space.sample()  #random action\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "          action = policy_network(state).argmax().item() #best action\n",
        "\n",
        "      #Perform the action\n",
        "      new_state,reward,terminated,truncated,_ = env.step(action)\n",
        "\n",
        "      # Save to Memory\n",
        "      memory.append((state,action,new_state,reward,terminated))\n",
        "\n",
        "      #Move to new state\n",
        "      state = new_state\n",
        "\n",
        "      # Increment step\n",
        "      step_count += 1\n",
        "\n",
        "      #Adding reward structure\n",
        "      if reward == 1:\n",
        "        rewards_per_episode[i]=1\n",
        "\n",
        "      #If enough experience has been collectd, and at least 1 reward has been collected, train\n",
        "\n",
        "      if len(memory)>self.mini_batch_size and np.sum(rewards_per_episode)>0:\n",
        "        mini_batch = memory.sample(self.mini_batch_size)\n",
        "        self.optimize(mini_batch,policy_network,target_network)  ##Define Optimize function\n",
        "\n",
        "        #epsilon decay\n",
        "        epsilon = max(epsilon-1/episodes,0)\n",
        "        epsilon_decays.append(epsilon)\n",
        "\n",
        "        if step_count > self.network_sync_rate:\n",
        "          target_network.load_state_dict(policy_network.state_dict())\n",
        "          step_count = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "Vsu27-BWSpK2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}